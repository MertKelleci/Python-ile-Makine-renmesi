{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab9c8c5-044d-4c45-af30-9bcad2008ed0",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\">Özet</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc96c40-e169-48f1-9c2e-109198e21f91",
   "metadata": {},
   "source": [
    "## Tahmin Modelleri:\n",
    "\n",
    "|Model|Artılar|Eksiler|\n",
    "|:---:|:---:|:---:|\n",
    "|**Linear Regression**|Veri boyutundan bağımsız olarak doğrusal ilişki üzerine kuruludur.|Doğrusallık kabulü aynı zamanda hatalıdır.|\n",
    "|**Polynomial Regression**|Doğrusal olmayan problemleri adresler.|Başarı için polinom derecesi önemlidir.|\n",
    "|**SVR**|Doğrusal olmayan modellerde çalışır, marjinal değerlere karşı ölçekleme ile dayanıklı olur.|Ölçekleme önemlidir, anlaşılması nispeten zordur, doğru kernel versiyonu seçimi önemlidir.|\n",
    "|**Decision Tree Regression**|Anlaşılabilirdir, ölçeklemeye ihtiyaç duymaz, doğrusal veya doğrusal olmayan problemlerde çalışır.|Sonuçlar sabitlenmiştir, küçük veri kümelerinde ezberleme olasılığı yüksektir.|\n",
    "|**Random Forest**|Ölçeklemeye ihtiyaç duymaz, doğrusal veya doğrusal olmayan problemlerde çalışır, ezber ve sabit sonuç riski düşüktür.|Çıktıların yorumu ve görsellemesi nispeten zordur.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cde3c24-5cbd-4ed6-b81a-19ed55a05115",
   "metadata": {},
   "source": [
    "| **Model Type**                          | **Scaling Needed?**                | **Reason**                                                                                 |\n",
    "|-----------------------------------------|-------------------------------------|-------------------------------------------------------------------------------------------|\n",
    "| **Support Vector Machines (SVMs)**      | Yes                                 | SVMs rely on distance calculations; unscaled features can dominate and bias the solution. |\n",
    "| **Gradient Descent-based Models**       | Yes                                 | Gradient Descent is sensitive to feature magnitudes, affecting convergence and performance.|\n",
    "| **k-Nearest Neighbors (kNN)**           | Yes                                 | kNN uses distance to measure similarity; unscaled features can skew results.              |\n",
    "| **Principal Component Analysis (PCA)**  | Yes                                 | PCA depends on variance, which can be dominated by features with larger magnitudes.       |\n",
    "| **Linear Regression (without regularization)** | No                          | Scale affects coefficients but not the model's ability to fit data.                       |\n",
    "| **Linear Regression (with regularization)** | Yes                        | Regularization penalizes coefficients; unscaled features result in unequal penalization.  |\n",
    "| **Polynomial Regression**               | Sometimes                           | Scaling can help prevent numerical instability with high-degree polynomial terms.          |\n",
    "| **Decision Trees**                      | No                                  | Decision Trees split based on feature thresholds; scale doesn’t impact split decisions.   |\n",
    "| **Random Forests**                      | No                                  | Random Forests inherit Decision Trees' insensitivity to scaling.                          |\n",
    "| **Gradient Boosted Trees**              | No                                  | Similar to Decision Trees, scale doesn’t affect splits or performance.                    |\n",
    "| **Naive Bayes**                         | No                                  | Naive Bayes relies on probabilities, not feature magnitudes.                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170a536a-40f4-40df-a9c6-ac0ee14054a7",
   "metadata": {},
   "source": [
    "## Gradient Descent-based Models:\n",
    "\n",
    "\n",
    "| **Model Type**                    | **Example Models**                                                                                     | **Notes**                                                                                                  |\n",
    "|-----------------------------------|---------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|\n",
    "| **Linear Models**                 | - Linear Regression (large datasets or with regularization)                                             | Gradient Descent is used for regularization or large datasets. Small datasets often use closed-form solutions. |\n",
    "|                                   | - Logistic Regression                                                                                   | Minimizes the log-loss function; scaling features is crucial for efficient convergence.                   |\n",
    "| **Neural Networks**               | - Artificial Neural Networks (ANNs)                                                                     | Backpropagation and Gradient Descent adjust weights to minimize the loss function.                        |\n",
    "|                                   | - Convolutional Neural Networks (CNNs)                                                                  | Used for image processing tasks, trained with Gradient Descent.                                           |\n",
    "|                                   | - Recurrent Neural Networks (RNNs, LSTMs, GRUs)                                                         | Used for sequential data, trained with advanced Gradient Descent optimizers.                              |\n",
    "|                                   | - Deep Learning Models (Transformers, BERT, GPT)                                                        | Trained using optimizers like Adam, RMSProp, or SGD.                                                      |\n",
    "| **Regularized Models**            | - Ridge Regression (L2 Regularization)                                                                  | Minimizes squared loss with L2 penalty.                                                                   |\n",
    "|                                   | - Lasso Regression (L1 Regularization)                                                                  | Minimizes squared loss with L1 penalty, sometimes using coordinate descent.                               |\n",
    "| **Support Vector Machines (SVMs)**| - Kernel SVMs                                                                                           | Large-scale SVMs use Gradient Descent for kernel optimization.                                            |\n",
    "| **Clustering**                    | - K-Means Clustering                                                                                    | Iterative optimization minimizes intra-cluster distances, conceptually similar to Gradient Descent.       |\n",
    "| **Matrix Factorization Models**   | - Collaborative Filtering                                                                               | Used in recommendation systems; optimizes reconstruction errors.                                          |\n",
    "| **Probabilistic Models**          | - Bayesian Logistic Regression                                                                          | Uses Gradient Descent techniques with Bayesian priors.                                                   |\n",
    "| **Reinforcement Learning**        | - Policy Gradient Methods (e.g., REINFORCE, PPO)                                                        | Updates policies using Gradient Descent in reinforcement learning tasks.                                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c086501-45ba-41ae-b937-820856588d5c",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"CRISP-DM-Model-Taylor-2017.png\" alt=\"CRISP-DM Model\" width=\"500\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
